<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Autonomous Navigation and Environment Planning using SWARM Robotics</title>
    <style>
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            background-color: #121212;
            color: #E0E0E0;
            margin: 0;
            padding: 0;
            line-height: 1.6;
        }
        header {
            background-color: #1F2937;
            color: #E0E0E0;
            padding: 1rem;
            text-align: center;
            border-bottom: 2px solid #4B5563;
        }
        .subtitle {
            font-size: 0.85rem;
            color: #9CA3AF;
            margin-top: 0.5rem;
        }
        section {
            margin: 20px auto;
            max-width: 900px;
            padding: 10px;
        }
        .box {
            background-color: #1E293B;
            border: 1px solid #4B5563;
            border-radius: 8px;
            padding: 20px;
            margin-bottom: 20px;
            box-shadow: 0 4px 8px rgba(0, 0, 0, 0.5);
        }
        h2 {
            color: #93C5FD;
            margin-top: 0;
        }
        h3 {
            color: #93C5FD;
            margin-top: 0.5rem;
        }
        ul {
            padding-left: 20px;
            list-style-type: disc;
            color: #D1D5DB;
        }
        ul ul {
            list-style-type: circle;
        }
        ul li {
            margin-bottom: 10px;
        }
        a {
            color: #60A5FA;
            text-decoration: none;
        }
        a:hover {
            text-decoration: underline;
        }
        footer {
            background-color: #1F2937;
            color: #9CA3AF;
            text-align: center;
            padding: 1rem;
            border-top: 1px solid #4B5563;
        }
        .image-container {
            text-align: center;
            margin: 20px 0;
        }
        img {
            display: block;
            margin: 0 auto;
            max-width: 100%;
            height: auto;
            border-radius: 10px;
        }
        .image-caption {
            font-style: italic;
            color: #9CA3AF;
            margin-top: 10px;
        }
    </style>
</head>
<body>

<header>
    <h1>Autonomous Navigation and Environment Planning using SWARM Robotics</h1>
    <p class="subtitle">Project Members: Devkumar Ojha, Hari Sumant, Brendan Love<br>Fall 2024 - ECE 5554 - Computer Vision - Group 10</p>
</header>

<section>

    <div class="box">
        <h2>Abstract</h2>
        <ul>
            <li>We wanted to implement and improve the world of SWARM robotics using computer vision methodologies to detect obstacles and overcome them. We used a mixture of CNN algorithms and tranformers to achieve this. The results showed us that the implementation of Computer vision in the world of SWARM robotics is the future for identify various terrains, and used for operations such as Search and Rescue.</li>
        </ul>
    </div>

    <div class="box">
        <h2>Introduction</h2>
        <ul>
            <li>The dream for our project and the dream for SWARM robotics correlates to the California Forest fires. Search and rescue missions for the thousands people who are stuck in various dangerous environmental situation is the goal for this project. </li>
        </ul>
    </div>

    <div class="box">
        <h2>Problem Statement</h2>
        <p>We would like to use a swarm community of robots connected to a master control unit to navigate and map a course of obstacles through object detections protocol and predicting collisions of the swarm to complete the course. The swarm will use mounted cameras to map the layout of the test course while the master properly navigates each of the agents. This project can be extended to situations to map rooms for people needing assistance, search and rescue operations, and schedule movements for a community of robots.</p>
        <div class="image-container">
            <img src="swarm_robotics.webp" alt="SWARM Robotics Image">
            
        </div>
    </div>

    <div class="box">
        <h2>Approach</h2>
        <ul>
            <li>Multiple simple robots will be simulated in a testing arena with walls and obstacles to impede the path to the end. A master unit will be used to plan the paths for each robot to take, communicating simple directions, such as turns, to each of the robots to guide them through the field. The master unit will be fitted with a camera to make use of computer vision techniques to detect obstacles and positions of the robots in the field.</li>
            <li>The robotic side of the system will be achieved via simulation to limit complications regarding budget and physical system design. This way, the team can primarily focus on concepts related to the computer vision task. Each of the robots will be fitted with a camera to allow for image-based mapping of the simulated course. The simulation will be performed in MuJoCo to demonstrate the application of the swarm.</li>
            <li>For the vision application, we will make use of object detection protocol and depth estimation to fully map the course. The swarm agents will send images to the master unit for vision operations to limit the need for processing power on each agent. For object detection, we will try to make use of the You Only Look Once (YOLO) protocol and test the capability of the YOLO models vs the Single Shot Detector model to test limits of the efficiency of the system. For depth estimation, we will test stereovision algorithms vs image sequence depth estimation. To achieve the stereovision, we would include two cameras on each of the swarm agents. To achieve the image sequence depth estimation, we would compare current images to a preceding time-stamped image to estimate the depth of objects in the environment.</li>
        </ul>
    </div>

    <div class="box">
        <h2>Testing and Experimentation</h2>

        <h3>Object Detection Testing</h3>
        <ul>
            <li>Testing how well the robot can detect objects
                <ul>
                    <li>Place 1 object in front of the robot and test the following:
                        <ul>
                            <li>Object distance - Checking how far the object is at a given moment</li>
                            <li>Object size - Checking if the robot can recognize how large the objects are in the robot's field of vision</li>
                            <li>Object amount - Checking how many objects are present in the FOV</li>
                            <li>Time for detection - Checking if the robot can detect the object in the given time constraint post-processing</li>
                        </ul>
                    </li>
                    <li>Place multiple objects in front of the robot and test the following:
                        <ul>
                            <li>Object distance - Checking how far the different objects are at a given moment</li>
                            <li>Object size - Checking if the robot can recognize how large the objects are in the robot's field of vision</li>
                            <li>Object amount - Checking how many objects are present in the FOV</li>
                            <li>Time for detection - Checking if the robot can detect the object in the given time constraint post-processing</li>
                        </ul>
                    </li>
                </ul>
            </li>
            <li>Testing YOLO model vs Single Shot Detector
                <ul>
                    <li>Test each model on a single obstacle
                        <ul>
                            <li>Create a known obstacle in the imaging path of a robot</li>
                            <li>Record the processing time taken vs the accuracy of detection to identify the obstacle with each YOLO size</li>
                            <li>Record the processing time taken vs the accuracy of detection to identify the obstacle with Single Shot Detector</li>
                        </ul>
                    </li>
                    <li>Test each model on several obstacles
                        <ul>
                            <li>Create several known obstacles in the imaging path of a robot</li>
                            <li>Record the processing time taken vs the accuracy of detection with each YOLO size</li>
                            <li>Record the processing time taken vs the accuracy of detection with Single Shot Detector</li>
                        </ul>
                    </li>
                </ul>
            </li>
            <div class="image-container">
                <img src="yolo_arc.png" alt="YOLO arch">
                <p class="image-caption">YOLO algorithm model for object detection.</p>
            </div>
        </ul>

        <h3>SWARM Robotics Testing</h3>
        <ul>
            <li>Movement Testing (1 to 3 spaces [Predefined in programming])
                <ul>
                    <li>Move Forwards</li>
                    <li>Move Left</li>
                    <li>Move Right</li>
                    <li>Move Backwards</li>
                </ul>
            </li>
            <li>Object Detection
                <ul>
                    <li>Communication between robots for detection of the same objects and avoiding multiple entries of the same object</li>
                    <li>Recognizing each other and not classifying each other as obstacles</li>
                </ul>
            </li>
            <li>Swarm Correspondence
                <ul>
                    <li>Correspondence between robots about location, and the area of the arena</li>
                </ul>
            </li>
        </ul>

        <h3>Combination Testing</h3>
        <ul>
            <li>Movement:
                <ul>
                    <li>Evading obstacle - Checking if the robot can make any evasive maneuvers</li>
                    <li>Concise evasion - Checking if the robotâ€™s evasive maneuvers are ideal for evasion and do not go too far out of the way</li>
                </ul>
            </li>
        </ul>
    </div>

    <div class="box">
        <h2>Experiment Setup</h2>
        <p>The experiment involves an environment designed with walls, obstacles, and mobile robots. These robots will navigate the environment and interact with obstacles. The obstacles will include various objects such as wall sections, cones, solid blocks, and other relevant structures provided by the software to simulate real-world scenarios.</p>
        <p>To aid in navigation, a master camera will be positioned at the midpoint of the arena. This camera will capture the positions of both the robots and the obstacles and transmit this data to the robots, facilitating communication and coordination among them.</p>
    </div>

    <div class="box">
        <h2>Dataset</h2>
        <p>The dataset will be generated in a simulation environment and will consist of feature embeddings for both objects and robots. These feature embeddings will be used for:</p>
        <ul>
            <li>Obstacle Identification: Feature embeddings of objects like walls and cones will be used for detecting them as obstacles.</li>
            <li>Edge Detection: Embeddings will help perform edge detection on obstacles to assist in navigation.</li>
            <li>Robot Recognition: The robots will also have their own feature embeddings to differentiate them from the obstacles.</li>
        </ul>
        <p>Data collection will start with manual runs, using recordings from the master camera. The dataset will then be used to train a simple Convolutional Neural Network (CNN) for feature detection and recognition.</p>
        <p>Two approaches will be tested:</p>
        <ul>
            <li>CNN for Feature Detection: The CNN will be applied to both robots and obstacles.</li>
            <li>Hybrid Approach: The CNN will focus on identifying the robots, while edge detection will be used for identifying obstacles. A more effective approach will be implemented.</li>
        </ul>
    </div>

    <div class="box">
        <h2>Expected Results</h2>
        <p>The goal of the experiment is to implement feature extraction and recognition for swarm robots, enabling them to map and navigate unknown environments. This method has potential applications in industrial automation, where robots can autonomously map areas and create optimized paths for future operations.</p>
        <p>By the end of the experiment, the robots should be capable of:</p>
        <ul>
            <li>Detecting and identifying obstacles in an unknown environment.</li>
            <li>Locating themselves within the environment.</li>
            <li>Mapping the area for future navigation.</li>
        </ul>    
    </div>

    <div class="box">
        <h2>Uncertainties and Constraints</h2>
        <p>There are a few uncertain outcomes and constraints:</p>
        <ul>
            <li>Path Planning Efficiency: The effectiveness of the path planning algorithms and how well they perform in real-world operations is uncertain.</li>
            <li>Master Camera Configuration: The configuration of the master unit and how it interacts with the robots will need careful attention.</li>
            <li>Robot Awareness: It may be necessary to determine what additional sensors or components should be included in the robots to enhance their awareness of the surroundings, beyond the data provided by the master camera.</li>
        </ul>
    </div>

    <div class="box">
        <h2>Resources</h2>
        <ul>
            <li><a href="https://github.com/automaticdai/rpi-object-detection">Raspberry Pi Object Detection</a></li>
            <li><a href="https://www.digikey.com/en/maker/projects/how-to-perform-object-detection-with-tensorflow-lite-on-raspberry-pi/b929e1519c7c43d5b2c6f89984883588">DigiKey Object Detection</a></li>
            <li><a href="https://mujoco.org/">MuJoCo Simulation Environment</a></li>
            <li><a href="https://arxiv.org/pdf/1608.03610">Robotics Communication Research</a></li>
            <li><a href="https://caslab.ece.vt.edu/research">Swarm Research</a></li>
            <li><a href="https://docs.opencv.org/4.x/">OpenCV Research</a></li>
        </ul>
    </div>

</section>

<footer>
    <p>Virginia Tech - ECE 5554 Computer Vision - Group 10</p>
</footer>

</body>
</html>
