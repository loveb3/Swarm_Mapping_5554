<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Autonomous Navigation and Environment Planning using SWARM Robotics</title>
    <style>
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            background-color: #121212;
            color: #E0E0E0;
            margin: 0;
            padding: 0;
            line-height: 1.6;
        }
        header {
            background-color: #1F2937;
            color: #E0E0E0;
            padding: 1rem;
            text-align: center;
            border-bottom: 2px solid #4B5563;
        }
        .subtitle {
            font-size: 0.85rem;
            color: #9CA3AF;
            margin-top: 0.5rem;
        }
        section {
            margin: 20px auto;
            max-width: 900px;
            padding: 10px;
        }
        .box {
            background-color: #1E293B;
            border: 1px solid #4B5563;
            border-radius: 8px;
            padding: 20px;
            margin-bottom: 20px;
            box-shadow: 0 4px 8px rgba(0, 0, 0, 0.5);
        }
        h2 {
            color: #93C5FD;
            margin-top: 0;
        }
        h3 {
            color: #93C5FD;
            margin-top: 0.5rem;
        }
        ul {
            padding-left: 20px;
            list-style-type: disc;
            color: #D1D5DB;
        }
        ul ul {
            list-style-type: circle;
        }
        ul li {
            margin-bottom: 10px;
        }
        a {
            color: #60A5FA;
            text-decoration: none;
        }
        a:hover {
            text-decoration: underline;
        }
        footer {
            background-color: #1F2937;
            color: #9CA3AF;
            text-align: center;
            padding: 1rem;
            border-top: 1px solid #4B5563;
        }
        .image-container {
            text-align: center;
            margin: 20px 0;
        }
        img {
            display: block;
            margin: 0 auto;
            max-width: 100%;
            height: auto;
            border-radius: 10px;
        }
        .image-caption {
            font-style: italic;
            color: #9CA3AF;
            margin-top: 10px;
        }
    </style>
</head>
<body>

<header>
    <h1>Autonomous Navigation and Environment Planning using SWARM Robotics</h1>
    <p class="subtitle">Project Members: Devkumar Ojha, Hari Sumant, Brendan Love<br>Fall 2024 - ECE 5554 - Computer Vision - Group 10</p>
</header>

<section>

    <div class="box">
        <h2>Abstract</h2>
        <ul>
            <li>Humans interact with the space around them by using their senses to guide themselves. But what if there were an environment that one needed to navigate, but it was too dangerous or the person was compromised in some way? To implement the environmental mapping a convolutional neural network was used to identify objects, transformation techniques were used to identify how far an object was from the robot agent, and a depth first search algorithm was used to visit each position in a simulated arena. The implementation discussed was able to visit each node in a simulated environment, detect possible obstructions, and store the identified objects with reasonably accuracy.</li>
        </ul>
    </div>

    <div class="box">
        <h2>Introduction</h2>
        <ul>
            <li>The team wanted to implement and improve the world of SWARM robotics using computer vision methodologies to detect obstacles and overcome them. Search and rescue missions for the thousands people who are stuck in various dangerous situations can be considered the ultimate application, since the robots would map the various and identify the obstacles. The results showed that the integration Computer vision and SWARM robotics can be used for identification challenges in various terrains, and the robots were able to move past and map the whole area they were placed in. This is an evolving method of solving the problem of environmental mapping problems such as Search and Rescue operations, as it decreases the direct involvement of human lives, guiding disabled peoples through rooms, and more.</li>
        </ul>
    </div>

    <div class="box">
        <h2>Problem Statement</h2>
        <p>The team would like to use a swarm community of robots connected to a master control unit to navigate and map a course of obstacles through object detections protocol and predicting collisions of the swarm to complete the course. The swarm will use mounted cameras to map the layout of the test course while the master properly navigates each of the agents. This project can be extended to situations to map rooms for people needing assistance, search and rescue operations, and schedule movements for a community of robots.</p>
        <div class="image-container">
            <img src="swarm_robotics.webp" alt="SWARM Robotics Image">
            
        </div>
    </div>

    <div class="box">
        <h2>Approach</h2>
        <ul>
            <li>Multiple simple robots will be simulated in a testing arena with walls and obstacles to impede the path to the end. A master unit will be used to plan the paths for each robot to take, communicating simple directions, such as turns, to each of the robots to guide them through the field. The master unit will be fitted with a camera to make use of computer vision techniques to detect obstacles and positions of the robots in the field.</li>
            <li>The whole code can be split into 3 main sections:
                <ul>
                    <li>Object Detection - This is the main Computer Vision aspect of the project, where a trained model is used to identify the obstacle that would be in front of the robot. To achieve good and consistent results, ResNet50 was used to get a base model that works well. Then model was then trained using Pascal VOC 2012, with all 20 of the classes included in the model. It was trained for 50 epochs, with the best resulting Model having an accuracy of ~91%. The model is then loaded into the main environment mapping program. The model is used when any of the robots encounter an obstacle, where the model will give confience scores for the objects detected in the image. The detecting program is restricted to 3 classes for testing purposes. This is due to the testing images being from a real-world grid environment, since that would give a more realistic indoor environment to the actual eventual application. The output that is fed into the environmental mapping is a results tuple, that contains the Label/Objects detected with the confidence scores for the 3 restricted classes for the set test case, being "Chair", "DiningTable" and "Sofa".</li>
                    <li>Depth Estimation - </li>
                    <li>Search Algorithm - The search algorithm is the backbone of the robotic exploration. The algorithm the team chose to use was a Depth First Search algorithm, to ensure that each node was visited in a manner that would make sense in a physical situation. When a robot reachs a node, it will use the aforementioned depth estimation code to determine if movement is possible to the front, left, or right or the current position. In this simulated environment, the bounds of the arena are known and directly passed in to the detection code to help determine the valid movements. The movements are then used to search for the next nodes in the grid to explore. As long as these nodes are not visited, the robot will attach the node to its frontier. If the node is obstructed, meaning that the direction is not a valid movement, it will attempt to identify the object in the way. This is accomplished using the aforementioned object detection code. The results of the object detection will be appended to the obstacle node. This set of commands repeats for each robot in the swarm for each loop. The loop will repeat for as long as any robot has nodes in its frontier. Once a robot has an empty frontier, it will remain dormant. The code ends once each robot has finished exploration.</li>
                </ul>

            </li>
        </ul>
    </div>

    <div class="box">
        <h2>Experimentation and Results</h2>
        <h3>Object Detection Testing</h3>
        <ul>
            <li>Testing and training the model was a straight forward process. The Pascal VOC 2012 dataset was split into the Training Dataset and the Validation dataset. </li>
        </ul>
        <h3>Depth Estimation Testing</h3>
        <ul>
            
        </ul>
        <h3>Depth First Search Algorithm Testing</h3>
        <ul>
            <li></li>
        </ul>
        <h3>SWARM Sizing Impact on Exploration Time</h3>
        <ul>
            <li></li>
        </ul>

    </div>

    <div class="box">
        <h2>Intended Testing and Experimentation</h2>
        <h3>These were the initial intended experiments, but as the project progressed, the team deviated and made major modifications to the overall architecture. These are left here are artifacts of proposal, these can be used for future developments and proper reporting of the project.</h3>
        <h3>Object Detection Testing</h3>
        <ul>
            <li>Testing how well the robot can detect objects
                <ul>
                    <li>Place 1 object in front of the robot and test the following:
                        <ul>
                            <li>Object distance - Checking how far the object is at a given moment</li>
                            <li>Object size - Checking if the robot can recognize how large the objects are in the robot's field of vision</li>
                            <li>Object amount - Checking how many objects are present in the FOV</li>
                            <li>Time for detection - Checking if the robot can detect the object in the given time constraint post-processing</li>
                        </ul>
                    </li>
                    <li>Place multiple objects in front of the robot and test the following:
                        <ul>
                            <li>Object distance - Checking how far the different objects are at a given moment</li>
                            <li>Object size - Checking if the robot can recognize how large the objects are in the robot's field of vision</li>
                            <li>Object amount - Checking how many objects are present in the FOV</li>
                            <li>Time for detection - Checking if the robot can detect the object in the given time constraint post-processing</li>
                        </ul>
                    </li>
                </ul>
            </li>
            <li>Testing YOLO model vs Single Shot Detector
                <ul>
                    <li>Test each model on a single obstacle
                        <ul>
                            <li>Create a known obstacle in the imaging path of a robot</li>
                            <li>Record the processing time taken vs the accuracy of detection to identify the obstacle with each YOLO size</li>
                            <li>Record the processing time taken vs the accuracy of detection to identify the obstacle with Single Shot Detector</li>
                        </ul>
                    </li>
                    <li>Test each model on several obstacles
                        <ul>
                            <li>Create several known obstacles in the imaging path of a robot</li>
                            <li>Record the processing time taken vs the accuracy of detection with each YOLO size</li>
                            <li>Record the processing time taken vs the accuracy of detection with Single Shot Detector</li>
                        </ul>
                    </li>
                </ul>
            </li>
            <div class="image-container">
                <img src="yolo_arc.png" alt="YOLO arch">
                <p class="image-caption">YOLO algorithm model for object detection.</p>
            </div>
        </ul>

        <h3>SWARM Robotics Testing</h3>
        <ul>
            <li>Movement Testing (1 to 3 spaces [Predefined in programming])
                <ul>
                    <li>Move Forwards</li>
                    <li>Move Left</li>
                    <li>Move Right</li>
                    <li>Move Backwards</li>
                </ul>
            </li>
            <li>Object Detection
                <ul>
                    <li>Communication between robots for detection of the same objects and avoiding multiple entries of the same object</li>
                    <li>Recognizing each other and not classifying each other as obstacles</li>
                </ul>
            </li>
            <li>Swarm Correspondence
                <ul>
                    <li>Correspondence between robots about location, and the area of the arena</li>
                </ul>
            </li>
        </ul>

        <h3>Combination Testing</h3>
        <ul>
            <li>Movement:
                <ul>
                    <li>Evading obstacle - Checking if the robot can make any evasive maneuvers</li>
                    <li>Concise evasion - Checking if the robot’s evasive maneuvers are ideal for evasion and do not go too far out of the way</li>
                </ul>
            </li>
        </ul>
    </div>

    <div class="box">
        <h2>Experiment Setup</h2>
        <p>The experiment involves an environment designed with walls, obstacles, and mobile robots. These robots will navigate the environment and interact with obstacles. The obstacles will include various objects such as wall sections, cones, solid blocks, and other relevant structures provided by the software to simulate real-world scenarios.</p>
        <p>To aid in navigation, a master camera will be positioned at the midpoint of the arena. This camera will capture the positions of both the robots and the obstacles and transmit this data to the robots, facilitating communication and coordination among them.</p>
    </div>

    <div class="box">
        <h2>Dataset</h2>
        <p>The dataset will be generated in a simulation environment and will consist of feature embeddings for both objects and robots. These feature embeddings will be used for:</p>
        <ul>
            <li>Obstacle Identification: Feature embeddings of objects like walls and cones will be used for detecting them as obstacles.</li>
            <li>Edge Detection: Embeddings will help perform edge detection on obstacles to assist in navigation.</li>
            <li>Robot Recognition: The robots will also have their own feature embeddings to differentiate them from the obstacles.</li>
        </ul>
        <p>Data collection will start with manual runs, using recordings from the master camera. The dataset will then be used to train a simple Convolutional Neural Network (CNN) for feature detection and recognition.</p>
        <p>Two approaches will be tested:</p>
        <ul>
            <li>CNN for Feature Detection: The CNN will be applied to both robots and obstacles.</li>
            <li>Hybrid Approach: The CNN will focus on identifying the robots, while edge detection will be used for identifying obstacles. A more effective approach will be implemented.</li>
        </ul>
    </div>

    <div class="box">
        <h2>Expected Results</h2>
        <p>The goal of the experiment is to implement feature extraction and recognition for swarm robots, enabling them to map and navigate unknown environments. This method has potential applications in industrial automation, where robots can autonomously map areas and create optimized paths for future operations.</p>
        <p>By the end of the experiment, the robots should be capable of:</p>
        <ul>
            <li>Detecting and identifying obstacles in an unknown environment.</li>
            <li>Locating themselves within the environment.</li>
            <li>Mapping the area for future navigation.</li>
        </ul>    
    </div>

    <div class="box">
        <h2>Uncertainties and Constraints</h2>
        <p>There are a few uncertain outcomes and constraints:</p>
        <ul>
            <li>Path Planning Efficiency: The effectiveness of the path planning algorithms and how well they perform in real-world operations is uncertain.</li>
            <li>Master Camera Configuration: The configuration of the master unit and how it interacts with the robots will need careful attention.</li>
            <li>Robot Awareness: It may be necessary to determine what additional sensors or components should be included in the robots to enhance their awareness of the surroundings, beyond the data provided by the master camera.</li>
        </ul>
    </div>

    <div class="box">
        <h2>Resources</h2>
        <ul>
            <li><a href="https://github.com/automaticdai/rpi-object-detection">Raspberry Pi Object Detection</a></li>
            <li><a href="https://www.digikey.com/en/maker/projects/how-to-perform-object-detection-with-tensorflow-lite-on-raspberry-pi/b929e1519c7c43d5b2c6f89984883588">DigiKey Object Detection</a></li>
            <li><a href="https://mujoco.org/">MuJoCo Simulation Environment</a></li>
            <li><a href="https://arxiv.org/pdf/1608.03610">Robotics Communication Research</a></li>
            <li><a href="https://caslab.ece.vt.edu/research">Swarm Research</a></li>
            <li><a href="https://docs.opencv.org/4.x/">OpenCV Research</a></li>
        </ul>
    </div>

</section>

<footer>
    <p>Virginia Tech - ECE 5554 Computer Vision - Group 10</p>
</footer>

</body>
</html>
